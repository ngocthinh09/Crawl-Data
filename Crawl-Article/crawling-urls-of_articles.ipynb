{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "main_page_url = \"https://vnexpress.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_year_month(date_begin, date_end):\n",
    "    return [(date_end.year - date_begin.year), (date_end.month - date_begin.month)]\n",
    "\n",
    "def chooseDate(date_begin, current_date, driver):\n",
    "    date_input = driver.find_element(by = By.CSS_SELECTOR, value = \"a.text-calendar.view_by_date\")\n",
    "    date_input.click()\n",
    "\n",
    "    [diff_year, diff_month] = diff_year_month(date_begin, current_date)\n",
    "    \n",
    "    if (diff_year > 0):\n",
    "        button_click = driver.find_element(by = By.CSS_SELECTOR, value = \"span.arrowDown\")\n",
    "        for i in range(diff_year):\n",
    "            button_click.click()\n",
    "            \n",
    "    if (diff_year < 0):\n",
    "        button_click = driver.find_element(by = By.CSS_SELECTOR, value = \"span.arrowUp\")\n",
    "        for i in range(-diff_year):\n",
    "            button_click.click()\n",
    "        \n",
    "    if (diff_month > 0):\n",
    "        button_click = driver.find_element(by = By.CSS_SELECTOR, value = \"span.flatpickr-prev-month\")\n",
    "        for i in range(diff_month):\n",
    "            button_click.click()\n",
    "    \n",
    "    if (diff_month < 0):\n",
    "        button_click = driver.find_element(by = By.CSS_SELECTOR, value = \"span.flatpickr-next-month\")\n",
    "        for i in range(-diff_month):\n",
    "            button_click.click()\n",
    "\n",
    "    days = driver.find_elements(by = By.CSS_SELECTOR, value = \"span.flatpickr-day\")\n",
    "    for day in days:\n",
    "        if (day.text == str(date_begin.day)):\n",
    "            day.click()\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_link_article_category(category : str, numUrls : int = 10, date_begin = datetime(2023, 1, 1), date_end = datetime(2023, 12, 31)):\n",
    "    url_to_category = main_page_url + '/' + category\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    driver.get(url_to_category)\n",
    "    \n",
    "    current_date = date_begin\n",
    "    days_list = []\n",
    "    while (current_date <= date_end):\n",
    "        days_list.append(current_date)\n",
    "        current_date += timedelta(days = 1)\n",
    "    random.shuffle(days_list)\n",
    "    days_list = tuple(days_list)\n",
    "    \n",
    "    list_urls = []\n",
    "    pbar = tqdm(total = numUrls, desc = \"Crawling Progress\", colour = \"cyan\", unit = \"url\")\n",
    "    for day in days_list:\n",
    "        if (len(list_urls) == numUrls):\n",
    "            break\n",
    "        current_date = datetime.now()\n",
    "        chooseDate(day, current_date, driver)\n",
    "        chooseDate(day, day, driver)\n",
    "        \n",
    "        link_articles = driver.find_elements(by = By.CSS_SELECTOR, value = \"article.item-news.item-news-common > h3.title-news > a\")\n",
    "        needs = min(len(link_articles), numUrls - len(list_urls), 3)    \n",
    "        for i in range(needs):\n",
    "            list_urls.append(link_articles[i].get_attribute(\"href\"))\n",
    "        pbar.update(needs)\n",
    "        \n",
    "    contentOfjson = {\n",
    "        \"category\"  : category,\n",
    "        \"list_urls\" : list_urls\n",
    "    }\n",
    "    \n",
    "    file_path = \"urls_of_articles/\" + category + \".json\" \n",
    "    with open(file_path, \"w\", encoding = \"utf-8\") as json_file:\n",
    "        json.dump(contentOfjson, json_file, ensure_ascii = False, indent = 4)\n",
    "        \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [08:32<00:00,  2.56s/url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1/7 categories : giao-duc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [08:35<00:00,  2.58s/url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 2/7 categories : phap-luat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [06:38<00:00,  1.99s/url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 3/7 categories : giai-tri\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [06:46<00:00,  2.03s/url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 4/7 categories : kinh-doanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [06:58<00:00,  2.09s/url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 5/7 categories : cong-nghe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [07:04<00:00,  2.12s/url]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 6/7 categories : the-thao\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Progress: 100%|\u001b[36m██████████\u001b[0m| 200/200 [06:38<00:00,  1.99s/url]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 7/7 categories : the-gioi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\"giao-duc\",\n",
    "              \"phap-luat\",\n",
    "              \"giai-tri\",\n",
    "              \"kinh-doanh\",\n",
    "              \"cong-nghe\",\n",
    "              \"the-thao\",\n",
    "              \"the-gioi\"]\n",
    "\n",
    "\n",
    "dt_begin = datetime(2023, 1, 1)\n",
    "dt_end = datetime(2023, 12, 31)\n",
    "\n",
    "for (i, category) in enumerate(categories):\n",
    "    crawl_link_article_category(category = category, numUrls = 200, date_begin = dt_begin, date_end = dt_end)\n",
    "    print(f\"Complete {i + 1}/{len(categories)} categories : {category}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
