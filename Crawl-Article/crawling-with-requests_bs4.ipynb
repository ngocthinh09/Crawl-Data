{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_article(url : str):\n",
    "    news = requests.get(url)\n",
    "    soup = BeautifulSoup(news.content, \"html.parser\")\n",
    "    \n",
    "    content = []\n",
    "    date = soup.select(\"span.date\")\n",
    "    for row_date in date:\n",
    "        content.append(row_date.text)\n",
    "    \n",
    "    title = soup.select(\"h1.title-detail\")\n",
    "    for row_title in title:\n",
    "        content.append(row_title.text)\n",
    "    \n",
    "    description = soup.select(\"p.description\")\n",
    "    for row_description in description:\n",
    "        content.append(row_description.text)\n",
    "        \n",
    "    body = soup.select(\"article.fck_detail > *\")\n",
    "    for element in body:\n",
    "        if (element.name == \"p\"):\n",
    "            content.append(element.text)\n",
    "        elif (element.name == \"table\"):\n",
    "            rows = element.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                content_in_row = []\n",
    "                for col in row.children:\n",
    "                    content_in_row.append(col.text.strip())\n",
    "                content_in_row = \" \".join(content_in_row)    \n",
    "                content.append(content_in_row)\n",
    "    content = \"\\n\".join(content)\n",
    "    \n",
    "    metadata = []\n",
    "    images = soup.select(\"article.fck_detail div.fig-picture img.lazy\")\n",
    "    for img in images:\n",
    "        src = img.get(\"src\", \"\") if (img.get(\"src\", \"\").startswith(\"https\")) else img.get(\"data-src\", \"\")\n",
    "        alt = img.get(\"alt\", \"\")\n",
    "        if not alt:\n",
    "            alt = \"No caption available\"\n",
    "        metadata.append(tuple([src, alt]))\n",
    "        \n",
    "    images_of_video = soup.select(\"article.fck_detail div.box_embed_video_parent.embed_video_new img\")\n",
    "    for img in images_of_video:\n",
    "        src = img.get(\"src\", \"\") if (img.get(\"src\", \"\").startswith(\"https\")) else img.get(\"data-src\", \"\")\n",
    "        alt = img.get(\"alt\", \"\")\n",
    "        if not alt:\n",
    "            alt = \"No caption available\"\n",
    "        metadata.append(tuple([src, alt]))\n",
    "    \n",
    "    data = {\n",
    "        \"url\" : url,\n",
    "        \"title\" : title[0].text.strip() if (title) else \"\",\n",
    "        \"content\" : content,\n",
    "        \"metadata\" : metadata,\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_urls_from_category(category : str):\n",
    "    with open(\"urls_of_articles/\" + category + \".json\", \"r\", encoding = \"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data['list_urls']\n",
    "\n",
    "def progress(category : str):\n",
    "    urls = read_urls_from_category(category)\n",
    "    numUrls = len(urls)\n",
    "    \n",
    "    progress_bar = tqdm(total = numUrls, desc = \"Crawling Progress\", colour = \"cyan\", unit = \"articles\")\n",
    "    for (i, url) in enumerate(urls):\n",
    "        if (category == \"cong-nghe\" and i <= 132):  continue\n",
    "        data = crawling_article(url)\n",
    "        file_save = \"VnExpress_Crawled_by_requests_bs4/\" + category + f\"/{i}.json\"\n",
    "        with open(file_save, \"w\", encoding=\"utf-8\") as json_file:  \n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "        progress_bar.update(1)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_of_main_page = \"https://vnexpress.net\"\n",
    "\n",
    "categories = [\"cong-nghe\",\n",
    "              \"the-thao\",\n",
    "              \"the-gioi\"]\n",
    "\n",
    "for (i, category) in enumerate(categories):\n",
    "    progress(category)\n",
    "    print(f\"Complete {i + 1}/{len(categories)} categories : {category}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://vnexpress.net/diem-uu-tien-ielts-cua-dai-hoc-cong-nghe-thong-tin-dai-hoc-quoc-gia-tp-hcm-4848706.html\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # with open(\"page-source.html\", \"w\", encoding = \"utf-8\") as file:\n",
    "# #     file.write(response.text)\n",
    "\n",
    "# soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# content = []\n",
    "\n",
    "# article = soup.select(\"article.fck_detail > *\")\n",
    "# for element in article:\n",
    "#     if (element.name == \"p\"):\n",
    "#         pass\n",
    "#     elif (element.name == \"table\"):\n",
    "#         rows = element.find_all(\"tr\")\n",
    "#         for row in rows:\n",
    "#             content_in_row = []\n",
    "#             for col in row.children:\n",
    "#                 content_in_row.append(col.text.strip())\n",
    "#             content_in_row = \" \".join(content_in_row)    \n",
    "#             content.append(content_in_row)\n",
    "            \n",
    "# for row in content:\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_crawl(url : str):\n",
    "#     news = requests.get(url)\n",
    "#     soup = BeautifulSoup(news.content, \"html.parser\")\n",
    "    \n",
    "#     content = []\n",
    "#     date = soup.select(\"span.date\")\n",
    "#     for row_date in date:\n",
    "#         content.append(row_date.text)\n",
    "    \n",
    "#     title = soup.select(\"h1.title-detail\")\n",
    "#     for row_title in title:\n",
    "#         content.append(row_title.text)\n",
    "    \n",
    "#     description = soup.select(\"p.description\")\n",
    "#     for row_description in description:\n",
    "#         content.append(row_description.text)\n",
    "        \n",
    "#     body = soup.select(\"article.fck_detail > *\")\n",
    "#     for element in body:\n",
    "#         if (element.name == \"p\"):\n",
    "#             content.append(element.text)\n",
    "#         elif (element.name == \"table\"):\n",
    "#             rows = element.find_all(\"tr\")\n",
    "#             for row in rows:\n",
    "#                 content_in_row = []\n",
    "#                 for col in row.children:\n",
    "#                     content_in_row.append(col.text.strip())\n",
    "#                 content_in_row = \" \".join(content_in_row)    \n",
    "#                 content.append(content_in_row)\n",
    "#     content = \"\\n\".join(content)\n",
    "    \n",
    "#     metadata = []\n",
    "#     images = soup.select(\"article.fck_detail div.fig-picture img.lazy\")\n",
    "#     for img in images:\n",
    "#         src = img.get(\"src\", \"\") if (img.get(\"src\", \"\").startswith(\"https\")) else img.get(\"data-src\", \"\")\n",
    "#         alt = img.get(\"alt\", \"\")\n",
    "#         if not alt:\n",
    "#             alt = \"No caption available\"\n",
    "#         metadata.append(tuple([src, alt]))\n",
    "        \n",
    "#     images_of_video = soup.select(\"article.fck_detail div.box_embed_video_parent.embed_video_new img\")\n",
    "#     for img in images_of_video:\n",
    "#         src = img.get(\"src\", \"\") if (img.get(\"src\", \"\").startswith(\"https\")) else img.get(\"data-src\", \"\")\n",
    "#         alt = img.get(\"alt\", \"\")\n",
    "#         if not alt:\n",
    "#             alt = \"No caption available\"\n",
    "#         metadata.append(tuple([src, alt]))\n",
    "    \n",
    "#     data = {\n",
    "#         \"url\" : url,\n",
    "#         \"title\" : title[0].text.strip() if (title) else \"\",\n",
    "#         \"content\" : content,\n",
    "#         \"metadata\" : metadata,\n",
    "#     }\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# url = \"https://vnexpress.net/diem-uu-tien-ielts-cua-dai-hoc-cong-nghe-thong-tin-dai-hoc-quoc-gia-tp-hcm-4848706.html\"\n",
    "# article = test_crawl(url)\n",
    "\n",
    "# with open(\"0.json\", \"w\", encoding=\"utf-8\") as json_file:  \n",
    "#     json.dump(article, json_file, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
